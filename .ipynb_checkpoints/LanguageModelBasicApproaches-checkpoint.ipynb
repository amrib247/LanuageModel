{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062460e-a7b6-4baf-8e75-ac985888c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d545-c815-430c-ad31-21d6ad1d1292",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines() # Load names into array of strings\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada45cf-d239-407f-94c3-95b9f1c482cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of words: ', len(words))\n",
    "print('MIN: ', min(len(w) for w in words))\n",
    "print('MAX: ', max(len(w) for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e3b98-0926-4dfe-a0fd-ede5cbca2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams (predict next character using the previous character)\n",
    "\n",
    "N = torch.zeros((27, 27), dtype = torch.int32) # Create 3 by 5 array of 32 bit integers, all zeroes, containing bigram counts\n",
    "chars = sorted(list(set(''.join(words)))) # Get all characters used in words into a list, in order'\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # Map characters to indices for 2d array storage\n",
    "stoi['.'] = 0 # Special Character for start or end of word\n",
    "itos = {i:s for s, i in stoi.items()} # Map indicies\n",
    "\n",
    "for w in words: # Iterate over all words\n",
    "    chs = ['.'] + list(w) + ['.'] # Adds special token/character to word to analyze which letters are more likely to start and end a word\n",
    "    for c1, c2 in zip(chs, chs[1:]): # Iterate over consecutive characters in word\n",
    "        N[stoi[c1], stoi[c2]] += 1\n",
    "        \n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e74150-c098-4d6f-b819-49e8739f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from bigram table\n",
    "\n",
    "p = N[0].float()\n",
    "p = p / p.sum() # Contains probability distrobution (Where first character is the start character (second character is the first character of the word)\n",
    "# Sample using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dcd26-a5ef-4c87-a5f8-8ee6db62daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization: Normalize each row of N\n",
    "P = (N+1).float() # Adding 1 removes all instances of impossible bigrams, still very unlikely\n",
    "\n",
    "for row in P:\n",
    "    row /= row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006b453-1615-4805-a753-f0001951117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Generate Names\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        p = P[idx] # Get row corresponding to probabilities of next character, given previous character at idx\n",
    "        \n",
    "        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b891cedb-f056-41cb-a51c-e9bc9efb5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4541)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood (measure how good the model is)\n",
    "# Checks original dataset and the probabilites of bigrams assigned byy model, sees how closely the predictions match the actual bigrams in the training\n",
    "# data. The likelihood of the model is all the probabilities of the bigrams found in the actual dataset multiplied, and we take the log of the\n",
    "# likelihood to better measure the sum of all the probabilities (they get too close to zero when multiplying a lot) \n",
    "# log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        log_likelihood += torch.log(P[idx1, idx2]) # Increment log_likelihood by log of each individual probability of a bigram in the data set\n",
    "        count += 1\n",
    "        #print('(', c1, c2, ')', 'Likelihood:', log_likelihood) \n",
    "\n",
    "loss = -log_likelihood / count # Calculate loss by averaging negative of log_likelihood over each bigram found (zero loss is ideal)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a8d1d32-74e7-413a-9643-7cdcc2e448c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Nueral Networks to guess next character given the previous one. With a nueral network, we can minimize the loss function using backtracking\n",
    "# Creating dataset: Get inputs and outputs\n",
    "\n",
    "xs, ys = [], [] # Inputs and outputs\n",
    "\n",
    "for w in words[:3]: # Iterate over all bigrams in all words\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]): \n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa7eb0-9ec4-4ce0-bad4-c61f0ceda036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Using one_hot encoding on training data\n",
    "# Converts 5 to [0,0,0,0,1,0 ... 0] array size is 27, as there are 27 possible values\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # numclasses is the size of the onehot encoding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08e574d0-c7a5-420d-9d42-e092c39438e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), requires_grad=True) # Weights of 27 neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a3e907e-4dd5-4165-a240-716007e0d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W # Martix multiplication, gets outputs of 27 nuerons\n",
    "# Output of  matrix multiplication is 5 X 27, 5 inputs and 27 results for each input\n",
    "\n",
    "counts = logits.exp() # exponentiate all results to get positive values, this is just like the N matrix in the bigram counting example\n",
    "prob = counts / counts.sum(1, keepdims=True) # Normalize each row of the matrix to have sum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28f43165-2ecc-4759-b275-bc05907cbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(3.9117, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Loss (without pytorch)\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for exp_y in ys:\n",
    "    log_likelihood += torch.log(prob[0, exp_y])\n",
    "    count += 1\n",
    "\n",
    "loss = -log_likelihood/count \n",
    "print('Loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "484506ac-b9a3-48ea-972b-3c6627a92967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num inputs: 228146\n"
     ]
    }
   ],
   "source": [
    "# Conpacted Setup\n",
    "import torch.nn.functional as F\n",
    "xs, ys = [], [] # Inputs and outputs\n",
    "num = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]): \n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('Num inputs:', num)\n",
    "W = torch.randn((27, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df01c653-03b0-433e-8b4c-f3d2975d4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: 2.45936918258667\n",
      "Round 2: 2.4593489170074463\n",
      "Round 3: 2.459329128265381\n",
      "Round 4: 2.4593093395233154\n",
      "Round 5: 2.459289789199829\n",
      "Round 6: 2.4592700004577637\n",
      "Round 7: 2.4592509269714355\n",
      "Round 8: 2.459231376647949\n",
      "Round 9: 2.4592125415802\n",
      "Round 10: 2.459193706512451\n",
      "Round 11: 2.459174633026123\n",
      "Round 12: 2.459156036376953\n",
      "Round 13: 2.459137439727783\n",
      "Round 14: 2.4591188430786133\n",
      "Round 15: 2.4591004848480225\n",
      "Round 16: 2.4590821266174316\n",
      "Round 17: 2.45906400680542\n",
      "Round 18: 2.459045886993408\n",
      "Round 19: 2.4590282440185547\n",
      "Round 20: 2.459010362625122\n",
      "Round 21: 2.4589927196502686\n",
      "Round 22: 2.458975315093994\n",
      "Round 23: 2.4589576721191406\n",
      "Round 24: 2.4589405059814453\n",
      "Round 25: 2.458922863006592\n",
      "Round 26: 2.4589059352874756\n",
      "Round 27: 2.4588892459869385\n",
      "Round 28: 2.4588723182678223\n",
      "Round 29: 2.458855628967285\n",
      "Round 30: 2.458838701248169\n",
      "Round 31: 2.458822250366211\n",
      "Round 32: 2.458805799484253\n",
      "Round 33: 2.458789348602295\n",
      "Round 34: 2.458773136138916\n",
      "Round 35: 2.458756923675537\n",
      "Round 36: 2.458740711212158\n",
      "Round 37: 2.4587252140045166\n",
      "Round 38: 2.4587090015411377\n",
      "Round 39: 2.458693027496338\n",
      "Round 40: 2.4586775302886963\n",
      "Round 41: 2.458662271499634\n",
      "Round 42: 2.458646535873413\n",
      "Round 43: 2.4586312770843506\n",
      "Round 44: 2.458615779876709\n",
      "Round 45: 2.4586005210876465\n",
      "Round 46: 2.458585500717163\n",
      "Round 47: 2.4585704803466797\n",
      "Round 48: 2.458555221557617\n",
      "Round 49: 2.458540916442871\n",
      "Round 50: 2.4585258960723877\n",
      "Round 51: 2.4585111141204834\n",
      "Round 52: 2.4584968090057373\n",
      "Round 53: 2.458481788635254\n",
      "Round 54: 2.458467721939087\n",
      "Round 55: 2.458453416824341\n",
      "Round 56: 2.4584391117095947\n",
      "Round 57: 2.4584248065948486\n",
      "Round 58: 2.4584109783172607\n",
      "Round 59: 2.4583969116210938\n",
      "Round 60: 2.458383083343506\n",
      "Round 61: 2.458369255065918\n",
      "Round 62: 2.45835542678833\n",
      "Round 63: 2.458341598510742\n",
      "Round 64: 2.4583280086517334\n",
      "Round 65: 2.4583144187927246\n",
      "Round 66: 2.458301067352295\n",
      "Round 67: 2.4582877159118652\n",
      "Round 68: 2.4582746028900146\n",
      "Round 69: 2.458261489868164\n",
      "Round 70: 2.4582479000091553\n",
      "Round 71: 2.458235025405884\n",
      "Round 72: 2.4582221508026123\n",
      "Round 73: 2.4582090377807617\n",
      "Round 74: 2.4581961631774902\n",
      "Round 75: 2.458183526992798\n",
      "Round 76: 2.4581706523895264\n",
      "Round 77: 2.458158016204834\n",
      "Round 78: 2.4581453800201416\n",
      "Round 79: 2.4581329822540283\n",
      "Round 80: 2.458120584487915\n",
      "Round 81: 2.4581079483032227\n",
      "Round 82: 2.4580960273742676\n",
      "Round 83: 2.458083391189575\n",
      "Round 84: 2.45807147026062\n",
      "Round 85: 2.458059549331665\n",
      "Round 86: 2.458047389984131\n",
      "Round 87: 2.4580349922180176\n",
      "Round 88: 2.4580233097076416\n",
      "Round 89: 2.4580116271972656\n",
      "Round 90: 2.4579997062683105\n",
      "Round 91: 2.4579880237579346\n",
      "Round 92: 2.4579765796661377\n",
      "Round 93: 2.4579648971557617\n",
      "Round 94: 2.457953453063965\n",
      "Round 95: 2.457941770553589\n",
      "Round 96: 2.457930088043213\n",
      "Round 97: 2.457918882369995\n",
      "Round 98: 2.4579076766967773\n",
      "Round 99: 2.4578964710235596\n",
      "Round 100: 2.457885265350342\n"
     ]
    }
   ],
   "source": [
    "# Tuning Nueral Network using back propogation\n",
    "for k in range(100):\n",
    "    \n",
    "    #Forward pass (Convert inputs, multiply inputs by weights, convert to probability distrobution)\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    prob = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -prob[torch.arange(num), ys].log().mean() # Calculate Loss in a better way using pytorch\n",
    "    print('Round', str(k + 1) + ':', loss.item())\n",
    "    \n",
    "    #Backward Pass\n",
    "    W.grad = None # Reset gradients \n",
    "    loss.backward() # Set gradients\n",
    "\n",
    "    # Update (Only 1 parameter to worry about tuning)\n",
    "    W.data += -50 * W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543385cc-631c-4d41-834c-c1b0cc039fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch venv",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
