{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4062460e-a7b6-4baf-8e75-ac985888c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b0d545-c815-430c-ad31-21d6ad1d1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines() # Load names into array of strings\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ada45cf-d239-407f-94c3-95b9f1c482cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  32033\n",
      "MIN:  2\n",
      "MAX:  15\n"
     ]
    }
   ],
   "source": [
    "print('Number of words: ', len(words))\n",
    "print('MIN: ', min(len(w) for w in words))\n",
    "print('MAX: ', max(len(w) for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0e3b98-0926-4dfe-a0fd-ede5cbca2546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0],\n",
       "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
       "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
       "            0,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
       "            0,   14,    2],\n",
       "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
       "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
       "            0,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "            0,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
       "            0,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
       "            0,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "            0, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
       "            0,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
       "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
       "            0,    0,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "            0,  215,   10],\n",
       "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
       "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
       "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
       "            0,  121,    0],\n",
       "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
       "            0,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
       "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams (predict next character using the previous character)\n",
    "\n",
    "N = torch.zeros((27, 27), dtype = torch.int32) # Create 3 by 5 array of 32 bit integers, all zeroes, containing bigram counts\n",
    "chars = sorted(list(set(''.join(words)))) # Get all characters used in words into a list, in order'\n",
    "\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # Map characters to indices for 2d array storage\n",
    "stoi['.'] = 0 # Special Character for start or end of word\n",
    "itos = {i:s for s, i in stoi.items()} # Map indicies\n",
    "\n",
    "for w in words: # Iterate over all words\n",
    "    chs = ['.'] + list(w) + ['.'] # Adds special token/character to word to analyze which letters are more likely to start and end a word\n",
    "    for c1, c2 in zip(chs, chs[1:]): # Iterate over consecutive characters in word\n",
    "        N[stoi[c1], stoi[c2]] += 1\n",
    "        \n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e74150-c098-4d6f-b819-49e8739f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from bigram table\n",
    "\n",
    "p = N[0].float()\n",
    "p = p / p.sum() # Contains probability distrobution (Where first character is the start character (second character is the first character of the word)\n",
    "# Sample using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394dcd26-a5ef-4c87-a5f8-8ee6db62daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization: Normalize each row of N\n",
    "P = (N+1).float() # Adding 1 removes all instances of impossible bigrams, still very unlikely\n",
    "\n",
    "for row in P:\n",
    "    row /= row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d006b453-1615-4805-a753-f0001951117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezitynn.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n",
      "da.\n",
      "staiyaubrtthrigotai.\n",
      "moliellavo.\n",
      "ke.\n",
      "teda.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Generate Names\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        p = P[idx] # Get row corresponding to probabilities of next character, given previous character at idx\n",
    "        \n",
    "        idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b891cedb-f056-41cb-a51c-e9bc9efb5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4255)\n"
     ]
    }
   ],
   "source": [
    "# Likelihood (measure how good the model is)\n",
    "# Checks original dataset and the probabilites of bigrams assigned byy model, sees how closely the predictions match the actual bigrams in the training\n",
    "# data. The likelihood of the model is all the probabilities of the bigrams found in the actual dataset multiplied, and we take the log of the\n",
    "# likelihood to better measure the sum of all the probabilities (they get too close to zero when multiplying a lot) \n",
    "# log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        log_likelihood += torch.log(P[idx1, idx2]) # Increment log_likelihood by log of each individual probability of a bigram in the data set\n",
    "        count += 1\n",
    "        #print('(', c1, c2, ')', 'Likelihood:', log_likelihood) \n",
    "\n",
    "loss = -log_likelihood / count # Calculate loss by averaging negative of log_likelihood over each bigram found (zero loss is ideal)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8d1d32-74e7-413a-9643-7cdcc2e448c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Nueral Networks to guess next character given the previous one. With a nueral network, we can minimize the loss function using backtracking\n",
    "# Creating dataset: Get inputs and outputs\n",
    "\n",
    "xs, ys = [], [] # Inputs and outputs\n",
    "\n",
    "for w in words[:3]: # Iterate over all bigrams in all words\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]): \n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6aa7eb0-9ec4-4ce0-bad4-c61f0ceda036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Using one_hot encoding on training data\n",
    "# Converts 5 to [0,0,0,0,1,0 ... 0] array size is 27, as there are 27 possible values\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # numclasses is the size of the onehot encoding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08e574d0-c7a5-420d-9d42-e092c39438e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27), requires_grad=True) # Weights of 27 neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3e907e-4dd5-4165-a240-716007e0d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W # Martix multiplication, gets outputs of 27 nuerons\n",
    "# Output of  matrix multiplication is 5 X 27, 5 inputs and 27 results for each input\n",
    "\n",
    "counts = logits.exp() # exponentiate all results to get positive values, this is just like the N matrix in the bigram counting example\n",
    "prob = counts / counts.sum(1, keepdims=True) # Normalize each row of the matrix to have sum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f43165-2ecc-4759-b275-bc05907cbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(3.9130, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Loss (without pytorch)\n",
    "log_likelihood = 0.0\n",
    "count = 0\n",
    "for exp_y in ys:\n",
    "    log_likelihood += torch.log(prob[0, exp_y])\n",
    "    count += 1\n",
    "\n",
    "loss = -log_likelihood/count \n",
    "print('Loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484506ac-b9a3-48ea-972b-3c6627a92967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num inputs: 228146\n"
     ]
    }
   ],
   "source": [
    "# Conpacted Setup\n",
    "import torch.nn.functional as F\n",
    "xs, ys = [], [] # Inputs and outputs\n",
    "num = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] \n",
    "    for c1, c2 in zip(chs, chs[1:]): \n",
    "        idx1 = stoi[c1]\n",
    "        idx2 = stoi[c2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('Num inputs:', num)\n",
    "W = torch.randn((27, 27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df01c653-03b0-433e-8b4c-f3d2975d4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Nueral Network using back propogation\n",
    "for k in range(100):\n",
    "    \n",
    "    #Forward pass (Convert inputs, multiply inputs by weights, convert to probability distrobution)\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    prob = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -prob[torch.arange(num), ys].log().mean() # Calculate Loss in a better way using pytorch\n",
    "    #print('Round', str(k + 1) + ':', loss.item())\n",
    "    \n",
    "    #Backward Pass\n",
    "    W.grad = None # Reset gradients \n",
    "    loss.backward() # Set gradients\n",
    "\n",
    "    # Update (Only 1 parameter to worry about tuning)\n",
    "    W.data += -50 * W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d1e2b2b-04eb-4ae9-a8af-e1ce50d8bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryillara\n",
      "h\n",
      "imiyalalemzaymaradiridiloyn\n",
      "nnico\n",
      "jonihitaan\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the Nueral Network\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        prob = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(prob, num_samples=1, replacement = True).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543385cc-631c-4d41-834c-c1b0cc039fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch venv",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
